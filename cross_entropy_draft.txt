
  <p>
    I won't delve into the math of the cross entropy loss, but here is an intuitive explanation of what it means. Feel free to skip it if you wish.
  </p>

  <p>
    The cross entropy is the average over the <variable>m</variable> examples of this product:
  </p>

  <p><tmi>
    -y \cdot \log(\hat{y})
  </tmi></p>

  <p>
    In the product above, <variable>y</variable> is a one hot encoded label, and <variable>ŷ</variable> is the label's inferred value. Remember that a loss measures how wrong the inferred value is. If <variable>ŷ</variable> points at the same label as <variable>y</variable>, then the loss is 0. The farther <variable>ŷ</variable> is from <variable>y</variable>, the bigger the loss.
  </p>

  <p>
    Now consider that, because of one hot encoding, <variable>y</variable> has one element per label---but only one of those elements contains a 1. The other elements are zeros, so they don't really matter when calculating the loss. Their contribution to the product <inlinecode>-y·log(ŷ)</inlinecode> is always 0, whatever the value of <variable>ŷ</variable>.
  </p>

  <p>
    For the element where <variable>y</variable> is 1, however, the matching element <variable>ŷ</variable> becomes important. If the classifier set it at 1 (meaning: “I'm pretty sure that this is the right label”), then we're golden: the loss becomes <inlinecode>-1·log(1)</inlinecode>, which is 0. Otherwise, <inlinecode>-1·log(ŷ)</inlinecode> becomes a positive number. Average those numbers over all the examples, and there you have it---our total loss.
  </p>

  <p>
    From now on, the cross entropy loss replaces the “old” log loss that I introduced in <ref linkend="sec.smoothing_it_out"/>. However, the two losses are closely related. If you like to play with math, you can even prove that the log loss is nothing but a binary version of the cross entropy loss: if you have only two labels, then the log loss and the cross entropy loss become identical.
  </p>
